import torch
import torch.nn as nn
import torch.utils.data as Data
import torchvision
import matplotlib.pyplot as plt
import os
import cv2
import numpy as np

# è®¾ç½®éšæœºç§å­ç¡®ä¿å¯å¤ç°æ€§
torch.manual_seed(1)

# è¶…å‚æ•°
EPOCH = 3  # è®­ç»ƒè½®æ•°
BATCH_SIZE = 50
LR = 0.001
DOWNLOAD_MNIST = True  # è‡ªåŠ¨ä¸‹è½½æ•°æ®é›†
MODEL_PATH = 'cnn2.pkl'  # æ¨¡å‹ä¿å­˜è·¯å¾„

# ä¸‹è½½/åŠ è½½MNISTæ•°æ®é›†
train_data = torchvision.datasets.MNIST(
    root='./data/',
    train=True,
    transform=torchvision.transforms.ToTensor(),
    download=DOWNLOAD_MNIST,
)

test_data = torchvision.datasets.MNIST(
    root='./data/',
    train=False,
    transform=torchvision.transforms.ToTensor()
)

# æ‰¹è®­ç»ƒåŠ è½½å™¨
train_loader = Data.DataLoader(
    dataset=train_data,
    batch_size=BATCH_SIZE,
    shuffle=True
)

# ä¿®å¤æµ‹è¯•æ•°æ®åŠ è½½ï¼ˆå…¼å®¹æ–°ç‰ˆæœ¬PyTorchï¼‰
test_x = test_data.data[:2000].unsqueeze(1).type(torch.FloatTensor) / 255.0
test_y = test_data.targets[:2000]


# å®šä¹‰CNNæ¨¡å‹
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 16, 5, 1, 2),
            nn.ReLU(),
            nn.MaxPool2d(2),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(16, 32, 5, 1, 2),
            nn.ReLU(),
            nn.MaxPool2d(2),
        )
        self.out = nn.Linear(32 * 7 * 7, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = x.view(x.size(0), -1)
        output = self.out(x)
        return output


# åˆå§‹åŒ–æ¨¡å‹ï¼ˆæ— CNNç»“æ„æ‰“å°ï¼‰
cnn = CNN()

# ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)
loss_func = nn.CrossEntropyLoss()

# ===================== æ–°å¢ï¼šè®°å½•è®­ç»ƒè¿‡ç¨‹çš„å‡†ç¡®ç‡å’ŒæŸå¤± =====================
train_metrics = {
    'epochs': [],  # è®°å½•è½®æ•°
    'steps': [],  # è®°å½•æ­¥æ•°
    'losses': [],  # è®°å½•æŸå¤±å€¼
    'accuracies': []  # è®°å½•å‡†ç¡®ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰
}

# å¼ºåˆ¶è®­ç»ƒæ¨¡å‹
print("\nğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
for epoch in range(EPOCH):
    for step, (b_x, b_y) in enumerate(train_loader):
        output = cnn(b_x)
        loss = loss_func(output, b_y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # æ¯50æ­¥æ‰“å°è®­ç»ƒçŠ¶æ€å¹¶è®°å½•æŒ‡æ ‡
        if step % 50 == 0:
            test_output = cnn(test_x)
            pred_y = torch.max(test_output, 1)[1].numpy()
            accuracy = float((pred_y == test_y.numpy()).sum()) / float(test_y.size(0)) * 100  # ç™¾åˆ†æ¯”

            # è®°å½•è®­ç»ƒæŒ‡æ ‡
            train_metrics['epochs'].append(epoch)
            train_metrics['steps'].append(step)
            train_metrics['losses'].append(loss.item())
            train_metrics['accuracies'].append(accuracy)

            # æ‰“å°è®­ç»ƒçŠ¶æ€ï¼ˆç™¾åˆ†æ¯”å‡†ç¡®ç‡ï¼‰
            print(f'Epoch: {epoch}/{EPOCH} | Step: {step} | Loss: {loss.item():.4f} | Test Acc: {accuracy:.2f}%')

# ä¿å­˜æ¨¡å‹
torch.save(cnn.state_dict(), MODEL_PATH)
print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜åˆ°ï¼š{MODEL_PATH}")

# æµ‹è¯•å‰32ä¸ªæ ·æœ¬
inputs = test_x[:32]
test_output = cnn(inputs)
pred_y = torch.max(test_output, 1)[1].numpy()
true_y = test_y[:32].numpy()

# æ‰“å°é¢„æµ‹ç»“æœ
print("\nğŸ“Š é¢„æµ‹ç»“æœï¼ˆå‰10ä¸ªï¼‰ï¼š")
print("é¢„æµ‹æ•°å­—:", pred_y[:10])
print("çœŸå®æ•°å­—:", true_y[:10])

# ===================== 1. ç»˜åˆ¶å‡†ç¡®ç‡éšè®­ç»ƒè¿‡ç¨‹å˜åŒ–æ›²çº¿ =====================
plt.figure(figsize=(10, 5))

# å­å›¾1ï¼šæŸå¤±å€¼å˜åŒ–
plt.subplot(1, 2, 1)
plt.plot(train_metrics['steps'], train_metrics['losses'], 'b-', linewidth=1.5, label='Training Loss')
plt.xlabel('Training Steps')
plt.ylabel('Loss Value')
plt.title('Training Loss Curve')
plt.grid(True, alpha=0.3)
plt.legend()

# å­å›¾2ï¼šå‡†ç¡®ç‡å˜åŒ–ï¼ˆæŒ‰epochæ ‡æ³¨ï¼‰
plt.subplot(1, 2, 2)
# ç»˜åˆ¶å‡†ç¡®ç‡æ›²çº¿
plt.plot(train_metrics['steps'], train_metrics['accuracies'], 'r-', linewidth=1.5, label='Test Accuracy')
# æ·»åŠ epochåˆ†éš”çº¿
epoch_steps = []
epoch_accs = []
for e in range(EPOCH):
    # æ‰¾åˆ°æ¯ä¸ªepochæœ€åä¸€æ­¥çš„ç´¢å¼•
    epoch_indices = [i for i, ep in enumerate(train_metrics['epochs']) if ep == e]
    if epoch_indices:
        last_idx = epoch_indices[-1]
        epoch_steps.append(train_metrics['steps'][last_idx])
        epoch_accs.append(train_metrics['accuracies'][last_idx])
        # ç»˜åˆ¶epochåˆ†éš”çº¿
        plt.axvline(x=train_metrics['steps'][last_idx], color='gray', linestyle='--', alpha=0.5)
        # æ ‡æ³¨epoch
        plt.text(train_metrics['steps'][last_idx], np.max(train_metrics['accuracies']),
                 f'Epoch {e + 1}', rotation=90, va='top', ha='right', fontsize=8)

plt.xlabel('Training Steps')
plt.ylabel('Accuracy (%)')
plt.title('Test Accuracy Curve (vs Training Steps)')
plt.ylim(0, 100)  # å‡†ç¡®ç‡èŒƒå›´0-100%
plt.grid(True, alpha=0.3)
plt.legend()

plt.suptitle('Training Metrics (Loss & Accuracy)', fontsize=12)
plt.tight_layout()
plt.savefig('training_accuracy_curve.png', dpi=150, bbox_inches='tight')
plt.show()

# ===================== 2. ç»˜åˆ¶å¸¦æ ‡æ³¨çš„32ä¸ªæ ·æœ¬é¢„æµ‹ç»“æœ =====================
plt.figure(figsize=(16, 8))  # è°ƒæ•´ç”»å¸ƒå¤§å°ä»¥å®¹çº³æ ‡æ³¨
n_rows = 4  # 32ä¸ªæ ·æœ¬åˆ†ä¸º4è¡Œ8åˆ—
n_cols = 8

# é€ä¸ªç»˜åˆ¶å›¾ç‰‡å¹¶æ·»åŠ æ ‡æ³¨
for i in range(32):
    plt.subplot(n_rows, n_cols, i + 1)
    # è·å–å•å¼ å›¾ç‰‡å¹¶è°ƒæ•´ç»´åº¦
    img = inputs[i].squeeze().numpy()
    plt.imshow(img, cmap='gray')

    # è®¾ç½®å›¾æ³¨ï¼šåŒºåˆ†é¢„æµ‹æ­£ç¡®/é”™è¯¯ï¼ˆä¸åŒé¢œè‰²ï¼‰
    true_label = true_y[i]
    pred_label = pred_y[i]
    if true_label == pred_label:
        # é¢„æµ‹æ­£ç¡®ï¼šç»¿è‰²æ ‡æ³¨
        title_text = f'True: {true_label}\nPred: {pred_label}'
        plt.title(title_text, color='green', fontsize=8)
    else:
        # é¢„æµ‹é”™è¯¯ï¼šçº¢è‰²æ ‡æ³¨
        title_text = f'True: {true_label}\nPred: {pred_label}'
        plt.title(title_text, color='red', fontsize=8)

    # éšè—åæ ‡è½´
    plt.xticks([])
    plt.yticks([])

# æ•´ä½“æ ‡é¢˜ï¼ˆç™¾åˆ†æ¯”å‡†ç¡®ç‡ï¼‰
correct = (pred_y == true_y).sum()
accuracy = correct / 32 * 100
plt.suptitle(f'MNIST prediction result (accuracy: {correct}/32 = {accuracy:.1f}%)', fontsize=14, y=0.98)
plt.tight_layout(rect=[0, 0, 1, 0.95])  # ç•™å‡ºé¡¶éƒ¨æ ‡é¢˜ç©ºé—´
plt.savefig('mnist_predictions_with_labels.png', dpi=150, bbox_inches='tight')
plt.show()

# ä¿ç•™OpenCVæ˜¾ç¤ºï¼ˆå¯é€‰ï¼‰
img_grid = torchvision.utils.make_grid(inputs, nrow=8, padding=2)
img_grid = img_grid.numpy().transpose(1, 2, 0)
img_grid = np.clip(img_grid, 0, 1)
cv2_img = (img_grid * 255).astype(np.uint8)
cv2_img = cv2.cvtColor(cv2_img, cv2.COLOR_RGB2BGR)

cv2.namedWindow('MNIST Predictions (Grid View)', cv2.WINDOW_NORMAL)
cv2.resizeWindow('MNIST Predictions (Grid View)', 800, 400)
cv2.imshow('MNIST Predictions (Grid View)', cv2_img)
print("\næŒ‰ä»»æ„é”®å…³é—­OpenCVçª—å£...")
cv2.waitKey(0)
cv2.destroyAllWindows()

# ===================== 3. é”™è¯¯æ ·æœ¬å•ç‹¬å¯è§†åŒ–ï¼ˆå¸¦æ ‡æ³¨ï¼‰ =====================
wrong_idx = np.where(pred_y != true_y)[0]
if len(wrong_idx) > 0:
    error_rate = len(wrong_idx) / len(pred_y) * 100
    print(f"\n num of false resultï¼š{len(wrong_idx)}ï¼Œerror rateï¼š{error_rate:.2f}%")

    # ç»˜åˆ¶é”™è¯¯æ ·æœ¬ï¼ˆæœ€å¤š16ä¸ªï¼‰
    plt.figure(figsize=(12, 6))
    n_wrong = min(len(wrong_idx), 16)
    n_wrong_rows = n_wrong // 4 if n_wrong % 4 == 0 else n_wrong // 4 + 1

    for i, idx in enumerate(wrong_idx[:16]):
        plt.subplot(n_wrong_rows, 4, i + 1)
        img = inputs[idx].squeeze().numpy()
        plt.imshow(img, cmap='gray')
        # çº¢è‰²æ ‡æ³¨é”™è¯¯æ ·æœ¬
        plt.title(f'True: {true_y[idx]}\nPred: {pred_y[idx]}', color='red', fontsize=10)
        plt.xticks([])
        plt.yticks([])

    plt.suptitle(f'é”™è¯¯é¢„æµ‹æ ·æœ¬ï¼ˆé”™è¯¯ç‡ï¼š{error_rate:.2f}%ï¼‰', fontsize=12, color='red')
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.savefig('mnist_wrong_predictions.png', dpi=150)
    plt.show()
else:
    print("\n å…¨éƒ¨é¢„æµ‹æ­£ç¡®ï¼å‡†ç¡®ç‡ï¼š100.00%")